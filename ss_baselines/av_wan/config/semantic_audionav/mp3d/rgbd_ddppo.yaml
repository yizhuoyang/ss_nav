# BASE_TASK_CONFIG_PATH: "configs/semantic_audionav/savi/mp3d/semantic_audiogoal_distractor.yaml"
BASE_TASK_CONFIG_PATH: "configs/semantic_audionav/savi/mp3d/semantic_audiogoal.yaml"

NUM_PROCESSES: 1
SENSORS: ["DEPTH_SENSOR", "RGB_SENSOR"]
NUM_UPDATES: 20000
LOG_INTERVAL: 10
CHECKPOINT_INTERVAL: 50
VIDEO_OPTION: []
VISUALIZATION_OPTION: []

DISTANCE_TO_GOAL:
  DISTANCE_TO: POINT

GEOMETRIC_MAP:
  MAP_SIZE: 400
  INTERNAL_MAP_SIZE: 1200
  MAP_RESOLUTION: 0.1
ACOUSTIC_MAP:
  MAP_SIZE: 20
  MAP_RESOLUTION: 1.0
ACTION_MAP:
  MAP_SIZE: 9
  MAP_RESOLUTION: 1.0


RL:
  PPO:
    clip_param: 0.2
    ppo_epoch: 2
    num_mini_batch: 2
    value_loss_coef: 0.5
    entropy_coef: 0.05
    lr: 2.5e-4
    eps: 1e-5
    max_grad_norm: 0.2
    # decide the length of history that ppo encodes
    num_steps: 150
    hidden_size: 512
    use_gae: True
    gamma: 0.99
    tau: 0.95
    use_linear_clip_decay: False
    use_linear_lr_decay: False
    # window size for calculating the past rewards
    reward_window_size: 50
    use_normalized_advantage: False
    policy_type: "smt"
    use_belief_predictor: False
    use_external_memory: True
    SCENE_MEMORY_TRANSFORMER:
      memory_size: 150
      hidden_size: 256
      nhead: 8
      num_encoder_layers: 1
      num_decoder_layers: 1
      dropout: 0.0
      activation: 'relu'
      use_pretrained: False
      pretrained_path: ''
      freeze_encoders: True
      pretraining: False
    BELIEF_PREDICTOR:
      online_training: True
      train_encoder: False
      lr: 0.001
      use_label_belief: True
      use_location_belief: True
      pretrained_weights: "data/models/mp3d/av_wan_new/data/ckpt.175.pth"
  DDPPO:
    sync_frac: 0.6
    distrib_backend: "GLOO"
    rnn_type: "GRU"
    num_recurrent_layers: 1
    backbone: "custom_resnet18"
    # choose the best pretrained SAVi based on validation curve
    pretrained_weights: "data/models/savi/data/ckpt.XXX.pth"
    pretrained: True
    reset_critic: False

# RL:
#   PPO:
#     # ppo params
#     clip_param: 0.2
#     ppo_epoch: 2
#     num_mini_batch: 2
#     value_loss_coef: 0.5
#     entropy_coef: 0.05
#     lr: 2.5e-4
#     eps: 1e-5
#     max_grad_norm: 0.2
#     # decide the length of history that ppo encodes
#     num_steps: 150
#     hidden_size: 512
#     use_gae: True
#     gamma: 0.99
#     tau: 0.95
#     use_linear_clip_decay: False
#     use_linear_lr_decay: False
#     # window size for calculating the past rewards
#     reward_window_size: 50
#     use_normalized_advantage: False
#   DDPPO:
#     sync_frac: 0.6
#     distrib_backend: "GLOO"
#     rnn_type: "GRU"
#     num_recurrent_layers: 1
#     backbone: "custom_resnet18"
#     reset_critic: True
